from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import os
from zipfile import ZipFile

# Initialize WebDriver with webdriver_manager and options
options = Options()
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# Navigate to the initial website
driver.get("http://www.biblioteca.presidencia.gov.br/presidencia/ex-presidentes/getulio-vargas")

# Wait for the page to load
time.sleep(2)

# Initialize list to hold discourse links
discourse_links = []

# Find years and get their links
soup = BeautifulSoup(driver.page_source, 'lxml')
year_links = [a['href'] for a in soup.select("a[data-mce-href*='resolveuid']")]

# Visit each year link
for year_link in year_links:
    driver.get(year_link)
    time.sleep(2)
    
    # Scrape all discourse links of the year
    soup = BeautifulSoup(driver.page_source, 'lxml')
    discourse_links += [a['href'] for a in soup.select("a.summary.url")]

# Download PDFs
for i, discourse_link in enumerate(discourse_links):
    driver.get(discourse_link)
    time.sleep(2)
    
    # Find the PDF link and download
    soup = BeautifulSoup(driver.page_source, 'lxml')
    pdf_link = soup.select_one("a[href*='.pdf']")['href']
    driver.get(pdf_link)
    time.sleep(2)
    
    # Stop after 35 downloads
    if i >= 34:
        break

# Locate the Downloads folder for the current OS
download_folder = os.path.expanduser("~") + "/Downloads/"

# Zip the downloaded PDFs
with ZipFile('discourses.zip', 'w') as myzip:
    for root, _, files in os.walk(download_folder):
        for file in files:
            if file.endswith(".pdf"):
                myzip.write(os.path.join(root, file), file)

# Close the driver
driver.quit()
